{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67331fde",
   "metadata": {},
   "source": [
    "#### Query Translation in Retrieval-Augmented Generation (RAG) pipeline refers to the process of transforming the initial query into various forms or sub-queries that can enhance the retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75db29b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Decomposition: Multi-query, Step-back, RAG-Fusion\n",
    "# Query Decomposition involves breaking down a complex query into simpler sub-queries. This can help in cases where a single query might be too broad or too complex for effective retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c162183",
   "metadata": {},
   "source": [
    "## Multi-Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81b04960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are the steps involved in saving data to VectorDB?', '2. Can you explain the process of storing information in VectorDB?', '3. How can I input data into VectorDB for storage?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import Packages\n",
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "import getpass\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "## initiate Vectordb here i have used persist vectordb which means i am loading the locally stored vectordb.\n",
    "vectordb_path = \"Vector_db\"\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectordb = Chroma(persist_directory=vectordb_path,\n",
    "                                       embedding_function=embeddings)\n",
    "vectorstore_retriever = vectordb.as_retriever(\n",
    "                search_kwargs={\n",
    "                    \"k\": 1\n",
    "                }\n",
    "            )\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "## Here we use MultiQueryRetriever from langcahin which will generate multiple queriesand retriever data accordingly.\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(), llm=llm\n",
    ")\n",
    "\n",
    "question = \"How to store the data into VectorDB?\"\n",
    "\n",
    "unique_docs = retriever_from_llm.invoke(question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc72bbc",
   "metadata": {},
   "source": [
    "### üîç Query Generation, Retrieval & Fusion Process\n",
    "\n",
    "- **Query Generation:**  \n",
    "  Generate multiple sub-queries from the user‚Äôs input to capture diverse perspectives and fully understand the user‚Äôs intent.\n",
    "\n",
    "- **Sub-query Retrieval:**  \n",
    "  Retrieve relevant information for each sub-query from large datasets and repositories, ensuring comprehensive and in-depth search results.\n",
    "\n",
    "- **Reciprocal Rank Fusion (RRF):**  \n",
    "  Merge the retrieved documents using *Reciprocal Rank Fusion (RRF)* to combine their ranks, prioritizing the most relevant and comprehensive results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8383d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Generated Sub-Queries:\n",
      "1. 1. What does LLM stand for?\n",
      "2. 2. Benefits of pursuing an LLM degree\n",
      "3. 3. Top universities offering LLM programs\n",
      "4. 4. Career opportunities for LLM graduates\n",
      "\n",
      "üìò Retrieved documents for query '1. What does LLM stand for?':\n",
      "- 3. Applications  LLMs: Used for a wide range of NLP tasks, from text generation and summarization to translation and sentiment analysis.  Transformers: Employed not just in NLP but also in other areas ...\n",
      "- 3. Applications  LLMs: Used for a wide range of NLP tasks, from text generation and summarization to translation and sentiment analysis.  Transformers: Employed not just in NLP but also in other areas ...\n",
      "\n",
      "üìò Retrieved documents for query '2. Benefits of pursuing an LLM degree':\n",
      "- 3. Applications  LLMs: Used for a wide range of NLP tasks, from text generation and summarization to translation and sentiment analysis.  Transformers: Employed not just in NLP but also in other areas ...\n",
      "- 3. Applications  LLMs: Used for a wide range of NLP tasks, from text generation and summarization to translation and sentiment analysis.  Transformers: Employed not just in NLP but also in other areas ...\n",
      "\n",
      "üìò Retrieved documents for query '3. Top universities offering LLM programs':\n",
      "- 3. Applications  LLMs: Used for a wide range of NLP tasks, from text generation and summarization to translation and sentiment analysis.  Transformers: Employed not just in NLP but also in other areas ...\n",
      "- 3. Applications  LLMs: Used for a wide range of NLP tasks, from text generation and summarization to translation and sentiment analysis.  Transformers: Employed not just in NLP but also in other areas ...\n",
      "\n",
      "üìò Retrieved documents for query '4. Career opportunities for LLM graduates':\n",
      "- 3. Applications  LLMs: Used for a wide range of NLP tasks, from text generation and summarization to translation and sentiment analysis.  Transformers: Employed not just in NLP but also in other areas ...\n",
      "- 3. Applications  LLMs: Used for a wide range of NLP tasks, from text generation and summarization to translation and sentiment analysis.  Transformers: Employed not just in NLP but also in other areas ...\n",
      "\n",
      "üîÅ Top 1 fused documents after RRF:\n",
      "1. Score=0.1322 | Snippet: 3. Applications\n",
      "\n",
      "LLMs: Used for a wide range of NLP tasks, from text generation and summarization to translation and sentiment analysis.\n",
      "\n",
      "Transformers...\n",
      "\n",
      "üí¨ Final Answer:\n",
      " LLM stands for Language Model (LM).\n"
     ]
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from operator import itemgetter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# üîπ STEP 1: Setup RAG-Fusion Query Generator\n",
    "# -----------------------------------------\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query.\n",
    "Generate 4 different search queries related to: {question}.\n",
    "Output only the queries, one per line:\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm_query_gen = ChatOpenAI(temperature=0)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | llm_query_gen\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: [q.strip() for q in x.split(\"\\n\") if q.strip()])\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# üîπ STEP 2: Load Local Chroma Vector DB\n",
    "# -----------------------------------------\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "persist_directory = \"local_vector_db\"\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "vectorstore_retriever = vectordb.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# üîπ STEP 3: Define Reciprocal Rank Fusion (RRF)\n",
    "# -----------------------------------------\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            fused_scores[doc_str] = fused_scores.get(doc_str, 0) + 1 / (rank + k)\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    return reranked_results\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# üîπ STEP 4: Build Retrieval Chain\n",
    "# -----------------------------------------\n",
    "retrieval_chain_rag_fusion = generate_queries | vectorstore_retriever.map() | reciprocal_rank_fusion\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# üîπ STEP 5: Run Query through RAG-Fusion\n",
    "# -----------------------------------------\n",
    "question = \"What is LLM?\"\n",
    "\n",
    "\n",
    "\n",
    "# Step 5a: Generate multiple sub-queries\n",
    "sub_queries = generate_queries.invoke({\"question\": question})\n",
    "print(\"\\nüß† Generated Sub-Queries:\")\n",
    "for i, q in enumerate(sub_queries, 1):\n",
    "    print(f\"{i}. {q}\")\n",
    "\n",
    "\n",
    "\n",
    "# Step 5b: Retrieve documents for each query\n",
    "retrieved_docs = []\n",
    "for q in sub_queries:\n",
    "    docs = vectorstore_retriever.invoke(q)\n",
    "    retrieved_docs.append(docs)\n",
    "    print(f\"\\nüìò Retrieved documents for query '{q}':\")\n",
    "    for d in docs:\n",
    "        print(\"-\", d.page_content[:200].replace(\"\\n\", \" \"), \"...\")  # show snippet\n",
    "\n",
    "\n",
    "\n",
    "# Step 5c: Apply Reciprocal Rank Fusion\n",
    "reranked_docs = reciprocal_rank_fusion(retrieved_docs)\n",
    "print(f\"\\nüîÅ Top {len(reranked_docs)} fused documents after RRF:\")\n",
    "for i, (doc, score) in enumerate(reranked_docs[:3], 1):  # limit to top 3\n",
    "    print(f\"{i}. Score={score:.4f} | Snippet: {doc.page_content[:150]}...\")\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# üîπ STEP 6: Final Answer Generation\n",
    "# -----------------------------------------\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm_answer = ChatOpenAI(temperature=0)\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": lambda _: \" \".join([doc.page_content for doc, _ in reranked_docs[:3]]),\n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm_answer\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_answer = final_rag_chain.invoke({\"question\": question})\n",
    "print(\"\\nüí¨ Final Answer:\\n\", final_answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "channel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
