{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36158258",
   "metadata": {},
   "source": [
    "### There are mianly three retrieval technique in RAG \n",
    "Here arer some mentioned below\n",
    "\n",
    "1) Exact match word\n",
    "2) Embedding match based\n",
    "3) Hybrid "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e9abca",
   "metadata": {},
   "source": [
    "### Exact match search - using simple approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3e025a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniln\\AppData\\Local\\Temp\\ipykernel_19960\\698202827.py:17: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n",
      "  class ExactMatchRetriever(BaseRetriever):\n",
      "2025-10-13 14:01:36,283 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " {'input': 'What is LlamaIndex?', 'context': [Document(metadata={}, page_content='Python is a programming language.'), Document(metadata={}, page_content='LlamaIndex is a framework for building LLM apps.'), Document(metadata={}, page_content='FAISS provides similarity search.')], 'answer': 'LlamaIndex is a framework for building LLM apps.'}\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Imports\n",
    "# -------------------------------\n",
    "from typing import List\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.schema import BaseRetriever\n",
    "from pydantic import PrivateAttr\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# -------------------------------\n",
    "# 1Ô∏è‚É£ Custom Exact Match Retriever\n",
    "# -------------------------------\n",
    "class ExactMatchRetriever(BaseRetriever):\n",
    "    _documents: List[Document] = PrivateAttr()\n",
    "\n",
    "    def __init__(self, documents: List[Document], **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._documents = documents\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        query_lower = query.lower()\n",
    "        return [\n",
    "            doc for doc in self._documents\n",
    "            if any(word in doc.page_content.lower() for word in query_lower.split())\n",
    "        ]\n",
    "\n",
    "# -------------------------------\n",
    "# 2Ô∏è‚É£ Prepare Documents\n",
    "# -------------------------------\n",
    "docs = [\n",
    "    Document(page_content=\"Python is a programming language.\"),\n",
    "    Document(page_content=\"LlamaIndex is a framework for building LLM apps.\"),\n",
    "    Document(page_content=\"FAISS provides similarity search.\"),\n",
    "]\n",
    "\n",
    "retriever = ExactMatchRetriever(docs)\n",
    "\n",
    "# -------------------------------\n",
    "# 3Ô∏è‚É£ Setup LLM + Prompt\n",
    "# -------------------------------\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the document combination chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# -------------------------------\n",
    "# 4Ô∏è‚É£ Create Retrieval Chain\n",
    "# -------------------------------\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# -------------------------------\n",
    "# 5Ô∏è‚É£ Query Example\n",
    "# -------------------------------\n",
    "query = \"What is LlamaIndex?\"\n",
    "result = chain.invoke({\"input\": query})\n",
    "\n",
    "print(\"Answer:\\n\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb91946",
   "metadata": {},
   "source": [
    "## Using BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6758e487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniln\\AppData\\Local\\Temp\\ipykernel_19960\\2147908413.py:23: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n",
      "  class BM25Retriever(BaseRetriever):\n",
      "2025-10-13 14:03:48,128 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " {'input': 'What is LlamaIndex?', 'context': [Document(metadata={}, page_content='Python is a programming language.'), Document(metadata={}, page_content='LlamaIndex is a framework for building LLM apps.'), Document(metadata={}, page_content='FAISS provides similarity search.')], 'answer': 'LlamaIndex is a framework for building LLM apps.'}\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.schema import BaseRetriever\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import PrivateAttr\n",
    "\n",
    "# -------------------------------\n",
    "# 3Ô∏è‚É£ Prepare Documents\n",
    "# -------------------------------\n",
    "docs = [\n",
    "    Document(page_content=\"Python is a programming language.\"),\n",
    "    Document(page_content=\"LlamaIndex is a framework for building LLM apps.\"),\n",
    "    Document(page_content=\"FAISS provides similarity search.\"),\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# 4Ô∏è‚É£ BM25 Retriever with PrivateAttr\n",
    "# -------------------------------\n",
    "class BM25Retriever(BaseRetriever):\n",
    "    _documents: List[Document] = PrivateAttr()\n",
    "    _tokenized_docs: List[List[str]] = PrivateAttr()\n",
    "    _bm25: BM25Okapi = PrivateAttr()\n",
    "\n",
    "    def __init__(self, documents: List[Document], **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._documents = documents\n",
    "        self._tokenized_docs = [doc.page_content.lower().split() for doc in documents]\n",
    "        self._bm25 = BM25Okapi(self._tokenized_docs)\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        query_tokens = query.lower().split()\n",
    "        scores = self._bm25.get_scores(query_tokens)\n",
    "        ranked_docs = [doc for _, doc in sorted(zip(scores, self._documents), reverse=True)]\n",
    "        return ranked_docs[:3]  # top 3 documents\n",
    "\n",
    "retriever = BM25Retriever(docs)\n",
    "\n",
    "# -------------------------------\n",
    "# 5Ô∏è‚É£ Setup LLM + Prompt\n",
    "# -------------------------------\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# -------------------------------\n",
    "# 6Ô∏è‚É£ Create Retrieval Chain\n",
    "# -------------------------------\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# -------------------------------\n",
    "# 7Ô∏è‚É£ Query Example\n",
    "# -------------------------------\n",
    "query = \"What is LlamaIndex?\"\n",
    "result = chain.invoke({\"input\": query})\n",
    "\n",
    "print(\"Answer:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3779948",
   "metadata": {},
   "source": [
    "### Ebmedding based "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4adc7583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniln\\AppData\\Local\\Temp\\ipykernel_19960\\3102930292.py:28: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n",
      "2025-10-13 14:07:06,672 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-10-13 14:07:06,711 - INFO - Loading faiss with AVX512 support.\n",
      "2025-10-13 14:07:06,713 - INFO - Could not load library with AVX512 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx512'\")\n",
      "2025-10-13 14:07:06,713 - INFO - Loading faiss with AVX2 support.\n",
      "2025-10-13 14:07:06,826 - INFO - Successfully loaded faiss with AVX2 support.\n",
      "2025-10-13 14:07:07,564 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-10-13 14:07:08,723 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " {'input': 'What is LlamaIndex?', 'context': [Document(id='7ed30682-61b3-4a7b-aa84-ebd6f6904ec5', metadata={}, page_content='LlamaIndex is a framework for building LLM apps.'), Document(id='c9a3998b-2597-487e-8a00-34a875259cc3', metadata={}, page_content='FAISS provides similarity search.'), Document(id='932a6196-1746-4466-bf40-d9f9a581c2ea', metadata={}, page_content='Python is a programming language.')], 'answer': 'LlamaIndex is a framework for building LLM apps.'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from getpass import getpass\n",
    "\n",
    "# -------------------------------\n",
    "# 3Ô∏è‚É£ Set OpenAI API Key\n",
    "# -------------------------------\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "# -------------------------------\n",
    "# 4Ô∏è‚É£ Prepare Documents\n",
    "# -------------------------------\n",
    "docs = [\n",
    "    Document(page_content=\"Python is a programming language.\"),\n",
    "    Document(page_content=\"LlamaIndex is a framework for building LLM apps.\"),\n",
    "    Document(page_content=\"FAISS provides similarity search.\"),\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# 5Ô∏è‚É£ Create embeddings and FAISS vector store\n",
    "# -------------------------------\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# -------------------------------\n",
    "# 6Ô∏è‚É£ Setup Retriever\n",
    "# -------------------------------\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}  # top 3 similar documents\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 7Ô∏è‚É£ Setup LLM and Prompt\n",
    "# -------------------------------\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentence maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# -------------------------------\n",
    "# 8Ô∏è‚É£ Create Retrieval Chain\n",
    "# -------------------------------\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# -------------------------------\n",
    "# 9Ô∏è‚É£ Query Example\n",
    "# -------------------------------\n",
    "query = \"What is LlamaIndex?\"\n",
    "response = chain.invoke({\"input\": query})\n",
    "\n",
    "print(\"Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7be85aa",
   "metadata": {},
   "source": [
    "### Hybrid Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c74036b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 14:10:38,425 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "C:\\Users\\aniln\\AppData\\Local\\Temp\\ipykernel_19960\\804332345.py:57: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n",
      "  class HybridRetriever(BaseRetriever):\n",
      "C:\\Users\\aniln\\AppData\\Local\\Temp\\ipykernel_19960\\804332345.py:68: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  embedding_docs = self._embedding_retriever.get_relevant_documents(query)\n",
      "2025-10-13 14:10:38,930 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-10-13 14:10:39,666 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " {'input': 'What is LlamaIndex?', 'context': [Document(metadata={}, page_content='Python is a programming language.'), Document(metadata={}, page_content='LlamaIndex is a framework for building LLM apps.'), Document(id='3ff3af9a-5d4e-4d36-9f92-f2ba9f83e205', metadata={}, page_content='FAISS provides similarity search.')], 'answer': 'LlamaIndex is a framework for building LLM apps.'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from typing import List\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.schema import BaseRetriever\n",
    "from pydantic import PrivateAttr\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "# -------------------------------\n",
    "# 3Ô∏è‚É£ Set OpenAI API Key\n",
    "# -------------------------------\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "# -------------------------------\n",
    "# 4Ô∏è‚É£ Prepare Documents\n",
    "# -------------------------------\n",
    "docs = [\n",
    "    Document(page_content=\"Python is a programming language.\"),\n",
    "    Document(page_content=\"LlamaIndex is a framework for building LLM apps.\"),\n",
    "    Document(page_content=\"FAISS provides similarity search.\"),\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# 5Ô∏è‚É£ BM25 Retriever\n",
    "# -------------------------------\n",
    "class BM25Retriever:\n",
    "    def __init__(self, documents: List[Document], top_k: int = 3):\n",
    "        self._documents = documents\n",
    "        self.top_k = top_k\n",
    "        self.tokenized_docs = [doc.page_content.lower().split() for doc in documents]\n",
    "        self.bm25 = BM25Okapi(self.tokenized_docs)\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        query_tokens = query.lower().split()\n",
    "        scores = self.bm25.get_scores(query_tokens)\n",
    "        ranked_docs = [doc for _, doc in sorted(zip(scores, self._documents), reverse=True)]\n",
    "        return ranked_docs[:self.top_k]\n",
    "\n",
    "# -------------------------------\n",
    "# 6Ô∏è‚É£ Embedding Retriever (FAISS)\n",
    "# -------------------------------\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "embedding_retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# -------------------------------\n",
    "# 7Ô∏è‚É£ Hybrid Retriever\n",
    "# -------------------------------\n",
    "class HybridRetriever(BaseRetriever):\n",
    "    _bm25_retriever: BM25Retriever = PrivateAttr()\n",
    "    _embedding_retriever: BaseRetriever = PrivateAttr()\n",
    "\n",
    "    def __init__(self, bm25_retriever: BM25Retriever, embedding_retriever: BaseRetriever, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._bm25_retriever = bm25_retriever\n",
    "        self._embedding_retriever = embedding_retriever\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        bm25_docs = self._bm25_retriever.get_relevant_documents(query)\n",
    "        embedding_docs = self._embedding_retriever.get_relevant_documents(query)\n",
    "        # Merge and remove duplicates\n",
    "        seen = set()\n",
    "        merged_docs = []\n",
    "        for doc in bm25_docs + embedding_docs:\n",
    "            if doc.page_content not in seen:\n",
    "                merged_docs.append(doc)\n",
    "                seen.add(doc.page_content)\n",
    "        return merged_docs\n",
    "\n",
    "# Instantiate hybrid retriever\n",
    "bm25_retriever = BM25Retriever(docs, top_k=2)\n",
    "retriever = HybridRetriever(bm25_retriever, embedding_retriever)\n",
    "\n",
    "# -------------------------------\n",
    "# 8Ô∏è‚É£ LLM and Prompt Setup\n",
    "# -------------------------------\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentence maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# -------------------------------\n",
    "# 9Ô∏è‚É£ Create Retrieval Chain\n",
    "# -------------------------------\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# -------------------------------\n",
    "# üîü Query Example\n",
    "# -------------------------------\n",
    "query = \"What is LlamaIndex?\"\n",
    "response = chain.invoke({\"input\": query})\n",
    "\n",
    "print(\"Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b096bc43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "channel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
