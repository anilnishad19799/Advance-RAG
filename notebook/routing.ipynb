{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bfea454",
   "metadata": {},
   "source": [
    "## üß≠ Routing in Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "Routing in RAG systems determines **how queries are processed** and **which retrievers, models, or tools** should handle them.  \n",
    "It ensures that user inputs are directed to the most suitable path for accurate and efficient responses.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 1. Semantic Routing\n",
    "Semantic Routing uses **embedding-based similarity** to understand the **meaning** of the query and decide where to route it.\n",
    "\n",
    "**How it works:**\n",
    "- The query is converted into an **embedding vector**.\n",
    "- The system compares it with predefined route embeddings (e.g., ‚Äúmath questions‚Äù, ‚Äúprogramming help‚Äù, ‚Äúdocument retrieval‚Äù).\n",
    "- The query is routed to the retriever or model with the **closest semantic match**.\n",
    "\n",
    "**Example:**\n",
    "- If the query is *\"Summarize the meeting notes\"*,  \n",
    "  it routes to the **document summarization chain**.\n",
    "- If the query is *\"Explain overfitting in machine learning\"*,  \n",
    "  it routes to the **knowledge retrieval chain**.\n",
    "\n",
    "**Use case:** Multi-domain RAG systems where queries can belong to diverse semantic categories.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 2. Logical Routing\n",
    "Logical Routing uses **explicit rules, conditions, or metadata** to direct the query flow.\n",
    "\n",
    "**How it works:**\n",
    "- Based on **if‚Äìelse logic** or **structured conditions** (e.g., metadata tags, keywords, document type).\n",
    "- Doesn‚Äôt rely on embeddings ‚Äî uses deterministic logic.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "if \"SQL\" in query:\n",
    "    route_to = \"Database_Retriever\"\n",
    "elif \"image\" in query:\n",
    "    route_to = \"Vision_Model\"\n",
    "else:\n",
    "    route_to = \"General_RAG_Chain\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c571244b",
   "metadata": {},
   "source": [
    "## Making two vector database one is for Medical and one is for legal documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70a755f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from uuid import uuid4\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aff3ba",
   "metadata": {},
   "source": [
    "### Medical Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2cb0481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniln\\AppData\\Local\\Temp\\ipykernel_15916\\307035922.py:5: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  medical_vector_store = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Added 53 text files to the vector database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniln\\AppData\\Local\\Temp\\ipykernel_15916\\307035922.py:32: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  medical_vector_store.persist()\n"
     ]
    }
   ],
   "source": [
    "# 2Ô∏è‚É£ Initialize OpenAI embeddings\n",
    "medical_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# 3Ô∏è‚É£ Create (or load existing) Chroma vector store\n",
    "medical_vector_store = Chroma(\n",
    "    collection_name=\"medical_rag_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"medical_collection\",\n",
    ")\n",
    "\n",
    "# 4Ô∏è‚É£ Read all .txt files and convert to Document objects\n",
    "medical_documents = []\n",
    "medical_text_folder = \"C:/Users/aniln/Desktop/github_celery_redis/Advance_RAG2\"\n",
    "filename = \"medical.txt\"\n",
    "file_path = os.path.join(medical_text_folder, filename)\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    texts = f.read().split(\"\\n\")\n",
    "    for text in texts:\n",
    "        doc = Document(\n",
    "            page_content=text.strip(),\n",
    "            metadata={\"source\": filename}\n",
    "        )\n",
    "        medical_documents.append(doc)\n",
    "\n",
    "# 5Ô∏è‚É£ Generate unique IDs for all documents\n",
    "uuids = [str(uuid4()) for _ in range(len(medical_documents))]\n",
    "\n",
    "# 6Ô∏è‚É£ Add documents to vector store\n",
    "medical_vector_store.add_documents(documents=medical_documents, ids=uuids)\n",
    "\n",
    "# 7Ô∏è‚É£ Persist (save) the database\n",
    "medical_vector_store.persist()\n",
    "\n",
    "print(f\"‚úÖ Added {len(medical_documents)} text files to the vector database.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a334dd5",
   "metadata": {},
   "source": [
    "### Sport Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9fedd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Added 521 text files to the vector database.\n"
     ]
    }
   ],
   "source": [
    "# 1Ô∏è‚É£ Define your local folder containing .txt files\n",
    "sport_text_folder = \"sport.txt\"  # e.g., ./text_data/myfile.txt\n",
    "\n",
    "# 2Ô∏è‚É£ Initialize OpenAI embeddings\n",
    "sport_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# 3Ô∏è‚É£ Create (or load existing) Chroma vector store\n",
    "\n",
    "sport_vector_store = Chroma(\n",
    "    collection_name=\"sport_rag_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"sport_collection\",\n",
    ")\n",
    "\n",
    "# 4Ô∏è‚É£ Read all .txt files and convert to Document objects\n",
    "sport_text_folder = \"C:/Users/aniln/Desktop/github_celery_redis/Advance_RAG2\"\n",
    "filename = \"sport.txt\"\n",
    "sport_documents = []\n",
    "file_path = os.path.join(sport_text_folder, filename)\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    texts = f.read().split(\"\\n\")\n",
    "\n",
    "    for text in texts:\n",
    "        doc = Document(\n",
    "            page_content=text.strip(),\n",
    "            metadata={\"source\": filename}\n",
    "        )\n",
    "        sport_documents.append(doc)\n",
    "\n",
    "\n",
    "# 5Ô∏è‚É£ Generate unique IDs for all documents\n",
    "uuids = [str(uuid4()) for _ in range(len(sport_documents))]\n",
    "\n",
    "# 6Ô∏è‚É£ Add documents to vector store\n",
    "sport_vector_store.add_documents(documents=sport_documents, ids=uuids)\n",
    "\n",
    "# 7Ô∏è‚É£ Persist (save) the database\n",
    "sport_vector_store.persist()\n",
    "\n",
    "print(f\"‚úÖ Added {len(sport_documents)} text files to the vector database.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705235a1",
   "metadata": {},
   "source": [
    "### Logical Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9add3562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniln\\AppData\\Local\\Temp\\ipykernel_15916\\453776759.py:38: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "Query: What are the symptoms of diabetes?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniln\\AppData\\Local\\Temp\\ipykernel_15916\\453776759.py:75: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  all_docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved Documents:\n",
      "[Doc 1] Source: medical.txt\n",
      "Additionally, the patient should know who to call in the event of an emergency. Many readers will note that these elements closely resemble a competency assessment; indeed, that is the point at hand. If the physician asks the patient the questions implied above, and records the patient's responses, monitoring of changes in the patient's condition may be delegated to that patient.\n",
      "--------------------------------------------------------------------------------\n",
      "[Doc 2] Source: medical.txt\n",
      "The last sovereign principle of documentation relates to the patient's capacity to participate in his or her own care. Examples of this include the patient's ability to understand the purposes of the various medications being prescribed, the patient's awareness of what symptoms to look for regarding exacerbation of the condition, and the patient's knowledge of what symptoms or states of mind constitute an emergency.\n",
      "--------------------------------------------------------------------------------\n",
      "[Doc 3] Source: medical.txt\n",
      "The second essential point of documentation is the use of clinical judgment at critical decision points. There are many possible definitions of clinical judgment, but a useful one for our purposes is ‚Äúan assessment of the clinical situation and a response congruent to that assessment.‚Äù\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniln\\AppData\\Local\\Temp\\ipykernel_15916\\453776759.py:97: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = llm.predict(prompt.format(context=context, question=query))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer: The text does not provide information on the symptoms of diabetes.\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Query: Who won the last football world cup?\n",
      "\n",
      "Retrieved Documents:\n",
      "[Doc 1] Source: sport.txt\n",
      "¬∑ World Cups\n",
      "--------------------------------------------------------------------------------\n",
      "[Doc 2] Source: sport.txt\n",
      "¬∑ FIH World Cup Qualifying Tournaments\n",
      "--------------------------------------------------------------------------------\n",
      "[Doc 3] Source: sport.txt\n",
      "¬∑ Junior World Cups\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Final Answer: The context does not provide information on the last football world cup winner.\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Query: What is the treatment for common cold?\n",
      "\n",
      "Retrieved Documents:\n",
      "[Doc 1] Source: medical.txt\n",
      "First, record the risk-benefit analysis of important decisions in the clinical care of the patient. This risk-benefit analysis should include even obvious or ‚Äúgiven‚Äù benefits. This is a point where many clinicians fall short because, in being risk-aversive, they tend to focus mostly on the risk side without equal attention to the benefit side of a decision. For example, prescribing a particular medication carries a certain risk of side effects, such as allergic reactions or other undesirable outcomes. Clinicians tend to focus on these possible risks and address them in particular in their record progress notes. However, the benefits of these medications are often stinted, and the risks of not receiving the medications are often omitted entirely. When the discussion of a decision, such as whether to medicate, includes both the risks and benefits of each path (prescribe or don't prescribe), the clinician's reasoning is viewed as far more reasonable than if only one of these elements was cited.\n",
      "--------------------------------------------------------------------------------\n",
      "[Doc 2] Source: medical.txt\n",
      "Thus, if the patient does in fact suffer an ill effect from medication, it is all too easy for a plaintiff's attorney to portray the physician as having cavalierly prescribed this dangerous substance to the patient; however, if the physician has noted that the goal of this medication is to prevent or improve a condition, the sense of a balanced and reasoned decision becomes much clearer to a lay audience.\n",
      "--------------------------------------------------------------------------------\n",
      "[Doc 3] Source: medical.txt\n",
      "It is important to identify the clinicians in question in medical records. When recording staff names, give a staff member's name and discipline. For example, ‚ÄúThe patient was medicated by R. Smithers, RN.‚Äù This completes the clear, concise format necessary to provide the relevant information in a given case.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Final Answer: The text does not provide information on the treatment for common cold.\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Query: List top cricket players in 2025.\n",
      "\n",
      "Retrieved Documents:\n",
      "[Doc 1] Source: sport.txt\n",
      "4. The instant compilation covers major sports played in India. The combat sports have\n",
      "--------------------------------------------------------------------------------\n",
      "[Doc 2] Source: sport.txt\n",
      "Note:- The above list is just for information. The Indian representative may change with time. There\n",
      "--------------------------------------------------------------------------------\n",
      "[Doc 3] Source: sport.txt\n",
      "6. Director / SE (Infrastructure) of Sports Authority of India has compiled the proposed\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Final Answer: The text does not provide information on the top cricket players in 2025.\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from uuid import uuid4\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# -------------------------------\n",
    "# 1Ô∏è‚É£ Initialize OpenAI embeddings\n",
    "# -------------------------------\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2Ô∏è‚É£ Load vector stores (collections)\n",
    "# -------------------------------\n",
    "medical_vector_store = Chroma(\n",
    "    collection_name=\"medical_rag_collection\",\n",
    "    persist_directory=\"medical_collection\",\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "sport_vector_store = Chroma(\n",
    "    collection_name=\"sport_rag_collection\",\n",
    "    persist_directory=\"sport_collection\",\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 3Ô∏è‚É£ Create retrievers for each domain\n",
    "# -------------------------------\n",
    "medical_retriever = medical_vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "sport_retriever = sport_vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# -------------------------------\n",
    "# 4Ô∏è‚É£ Initialize LLM\n",
    "# -------------------------------\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4\",  # or \"gpt-3.5-turbo\"\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 5Ô∏è‚É£ Define logical router\n",
    "# -------------------------------\n",
    "def route_query(query: str):\n",
    "    \"\"\"\n",
    "    Route query to the appropriate domain retriever based on keywords.\n",
    "    \"\"\"\n",
    "    medical_keywords = [\"disease\", \"treatment\", \"doctor\", \"symptom\", \"medical\", \"health\"]\n",
    "    sport_keywords = [\"football\", \"soccer\", \"cricket\", \"match\", \"tournament\", \"sport\"]\n",
    "\n",
    "    query_lower = query.lower()\n",
    "\n",
    "    if any(word in query_lower for word in medical_keywords):\n",
    "        return medical_retriever\n",
    "    elif any(word in query_lower for word in sport_keywords):\n",
    "        return sport_retriever\n",
    "    else:\n",
    "        # Default: search both and merge results\n",
    "        return [medical_retriever, sport_retriever]\n",
    "\n",
    "# -------------------------------\n",
    "# 6Ô∏è‚É£ Answer query with retrieved docs\n",
    "# -------------------------------\n",
    "def answer_query(query: str):\n",
    "    retriever = route_query(query)\n",
    "    \n",
    "    # Retrieve documents\n",
    "    if isinstance(retriever, list):\n",
    "        all_docs = []\n",
    "        for r in retriever:\n",
    "            all_docs.extend(r.get_relevant_documents(query))\n",
    "    else:\n",
    "        all_docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    # Print retrieved documents\n",
    "    print(\"\\nRetrieved Documents:\")\n",
    "    for i, doc in enumerate(all_docs, 1):\n",
    "        print(f\"[Doc {i}] Source: {doc.metadata.get('source', 'unknown')}\")\n",
    "        print(doc.page_content)\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Concatenate documents content as context\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in all_docs])\n",
    "\n",
    "    # Create prompt template for LLM\n",
    "    template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    # Run LLM\n",
    "    answer = llm.predict(prompt.format(context=context, question=query))\n",
    "    \n",
    "    # Return final answer\n",
    "    return answer\n",
    "\n",
    "# -------------------------------\n",
    "# 7Ô∏è‚É£ Test the routing\n",
    "# -------------------------------\n",
    "queries = [\n",
    "    \"What are the symptoms of diabetes?\",\n",
    "    \"Who won the last football world cup?\",\n",
    "    \"What is the treatment for common cold?\",\n",
    "    \"List top cricket players in 2025.\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"Query:\", q)\n",
    "    answer = answer_query(q)\n",
    "    print(\"\\nFinal Answer:\", answer)\n",
    "    print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fac4c51",
   "metadata": {},
   "source": [
    "### Semantic Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a63cba9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Query: What are the symptoms of diabetes?\n",
      "\n",
      "Retrieved Documents:\n",
      "[Doc 1] Source: medical.txt\n",
      "Additionally, the patient should know who to call in the event of an emergency. Many readers will note that these elements closely resemble a competency assessment; indeed, that is the point at hand. If the physician asks the patient the questions implied above, and records the patient's responses, monitoring of changes in the patient's condition may be delegated to that patient.\n",
      "--------------------------------------------------------------------------------\n",
      "[Doc 2] Source: medical.txt\n",
      "The last sovereign principle of documentation relates to the patient's capacity to participate in his or her own care. Examples of this include the patient's ability to understand the purposes of the various medications being prescribed, the patient's awareness of what symptoms to look for regarding exacerbation of the condition, and the patient's knowledge of what symptoms or states of mind constitute an emergency.\n",
      "--------------------------------------------------------------------------------\n",
      "[Doc 3] Source: medical.txt\n",
      "The second essential point of documentation is the use of clinical judgment at critical decision points. There are many possible definitions of clinical judgment, but a useful one for our purposes is ‚Äúan assessment of the clinical situation and a response congruent to that assessment.‚Äù\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Final Answer: The text does not provide information on the symptoms of diabetes.\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Query: Who won the last football world cup?\n",
      "\n",
      "Retrieved Documents:\n",
      "[Doc 1] Source: sport.txt\n",
      "¬∑ World Cups\n",
      "--------------------------------------------------------------------------------\n",
      "[Doc 2] Source: sport.txt\n",
      "¬∑ FIH World Cup Qualifying Tournaments\n",
      "--------------------------------------------------------------------------------\n",
      "[Doc 3] Source: sport.txt\n",
      "¬∑ Junior World Cups\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Final Answer: The context does not provide information on the last football world cup winner.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from uuid import uuid4\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from langchain.utils.math import cosine_similarity\n",
    "import numpy as np\n",
    "    \n",
    "\n",
    "# -------------------------------\n",
    "# 1Ô∏è‚É£ Initialize OpenAI embeddings\n",
    "# -------------------------------\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2Ô∏è‚É£ Load vector stores (collections)\n",
    "# -------------------------------\n",
    "medical_vector_store = Chroma(\n",
    "    collection_name=\"medical_rag_collection\",\n",
    "    persist_directory=\"medical_collection\",\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "sport_vector_store = Chroma(\n",
    "    collection_name=\"sport_rag_collection\",\n",
    "    persist_directory=\"sport_collection\",\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 3Ô∏è‚É£ Create retrievers\n",
    "# -------------------------------\n",
    "medical_retriever = medical_vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "sport_retriever = sport_vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# -------------------------------\n",
    "# 4Ô∏è‚É£ Initialize LLM\n",
    "# -------------------------------\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "\n",
    "# -------------------------------\n",
    "# 5Ô∏è‚É£ Precompute \"domain embeddings\"\n",
    "#    For example, each domain has a representative prompt\n",
    "# -------------------------------\n",
    "domain_prompts = {\n",
    "    \"medical\": \"Medical domain: diseases, symptoms, treatments, health, doctors\",\n",
    "    \"sport\": \"Sports domain: football, soccer, cricket, matches, tournaments, players\"\n",
    "}\n",
    "\n",
    "domain_embeddings = {\n",
    "    domain: embeddings.embed_query(text)\n",
    "    for domain, text in domain_prompts.items()\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# 6Ô∏è‚É£ Semantic routing based on query embedding\n",
    "# -------------------------------\n",
    "def semantic_route(query: str):\n",
    "    query_emb = np.array(embeddings.embed_query(query)).reshape(1, -1)  # make 2D\n",
    "    \n",
    "    # Compute cosine similarity safely\n",
    "    scores = {}\n",
    "    for domain, dom_emb in domain_embeddings.items():\n",
    "        dom_emb_2d = np.array(dom_emb).reshape(1, -1)\n",
    "        scores[domain] = cosine_similarity(query_emb, dom_emb_2d)[0][0]  # extract scalar\n",
    "\n",
    "    # Pick the domain with the highest similarity\n",
    "    best_domain = max(scores, key=scores.get)\n",
    "    \n",
    "    if best_domain == \"medical\":\n",
    "        return medical_retriever\n",
    "    else:\n",
    "        return sport_retriever\n",
    "\n",
    "# -------------------------------\n",
    "# 7Ô∏è‚É£ Answer query function\n",
    "# -------------------------------\n",
    "def answer_query(query: str):\n",
    "    retriever = semantic_route(query)\n",
    "    \n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    # Print retrieved docs\n",
    "    print(\"\\nRetrieved Documents:\")\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        print(f\"[Doc {i}] Source: {doc.metadata.get('source', 'unknown')}\")\n",
    "        print(doc.page_content)\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    # Use prompt template\n",
    "    template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "    answer = llm.predict(prompt.format(context=context, question=query))\n",
    "    return answer\n",
    "\n",
    "# -------------------------------\n",
    "# 8Ô∏è‚É£ Test\n",
    "# -------------------------------\n",
    "queries = [\n",
    "    \"What are the symptoms of diabetes?\",\n",
    "    \"Who won the last football world cup?\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Query:\", q)\n",
    "    answer = answer_query(q)\n",
    "    print(\"\\nFinal Answer:\", answer)\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b7ef90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "channel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
